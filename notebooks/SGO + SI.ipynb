{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn import BCELoss\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from transformers import *\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Get cuda if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "for index in range(n_gpu):\n",
    "    print(torch.cuda.get_device_name(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "csv_path = Path(\"path_to_CSVs\")\n",
    "delimiter='\\t' # default delimiter is comma\n",
    "df_train = pd.read_csv(csv_path/'train.csv', delimiter=delimiter)\n",
    "cols = df_train.columns\n",
    "label_cols = list(cols[3:])\n",
    "num_labels = len(label_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.load(f'dataloaders/train_data_loader-{batch_size}-{max_length}')\n",
    "validation_dataloader = torch.load(f'dataloaders/validation_data_loader-{batch_size}-{max_length}')\n",
    "test_dataloader = torch.load(f'dataloaders/test_data_loader-{batch_size}-{max_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    'train': train_dataloader,\n",
    "    'dev': validation_dataloader,\n",
    "    'test': test_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_threshold(true_labels, pred_labels_sigmoid):\n",
    "    \n",
    "    true_bools = [tl==1 for tl in true_labels]\n",
    "\n",
    "    #range of thresholds to test\n",
    "    micro_thresholds = (np.array(range(101))/100) \n",
    "\n",
    "    f1_results, acc_results = [], []\n",
    "    \n",
    "    for th in tqdm(micro_thresholds, desc=\"Best Threshold Calculation\", leave=False):\n",
    "        pred_bools = [pl>th for pl in pred_labels_sigmoid]\n",
    "        \n",
    "        micro_f1 = f1_score(true_bools,pred_bools,average='micro')*100\n",
    "        f1_results.append(micro_f1)\n",
    "\n",
    "\n",
    "    best_f1_idx = np.argmax(f1_results) #best threshold value\n",
    "\n",
    "    # Printing and saving classification report\n",
    "    best_threshold = micro_thresholds[best_f1_idx]\n",
    "    best_f1_score = f1_results[best_f1_idx]\n",
    "\n",
    "    best_pred_bools = [pl>best_threshold for pl in pred_labels_sigmoid]\n",
    "    \n",
    "    _, best_acc, precision, recall = get_metrics(true_bools, best_pred_bools)\n",
    "    \n",
    "    return best_threshold, best_f1_score, best_acc, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_thresholds(true_labels, pred_labels_sigmoid):\n",
    "    true_bools = [tl==1 for tl in true_labels]\n",
    "\n",
    "    micro_thresholds = (np.array(range(101))/100) #calculating micro threshold values\n",
    "\n",
    "    thresholds = []\n",
    "\n",
    "    for i in trange(len(label_cols), desc=\"Best Thresholds Calculation\", leave=False) :\n",
    "        f1_results, flat_acc_results = [], []\n",
    "        for th in micro_thresholds:\n",
    "            pred_bools = [pl>th for pl in pred_labels_sigmoid]\n",
    "            true_lab = [ labels[i] for labels in true_bools]\n",
    "            pred_lab = [ labels[i] for labels in pred_bools]\n",
    "\n",
    "            micro_f1 = f1_score(true_lab,pred_lab, average='micro', zero_division=1)\n",
    "            f1_results.append(micro_f1)\n",
    "\n",
    "        best_f1_idx = np.argmax(f1_results) #best threshold value\n",
    "\n",
    "        thresholds.append(micro_thresholds[best_f1_idx])\n",
    "\n",
    "    best_pred_bools = [ [e>thresh for e,thresh in zip(pl,thresholds)] for pl in pred_labels_sigmoid] \n",
    "    \n",
    "    micro_f1_score, accuracy, precision, recall = get_metrics(true_bools, best_pred_bools)\n",
    "    return thresholds, micro_f1_score, accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(method, f1, acc, precision, recall):\n",
    "    print('\\n'+method+' :')\n",
    "    print('Micro F1-Score =', f1)\n",
    "    print('Accuracy =', acc)\n",
    "    print('Micro Avg : precision =', precision, 'recall =', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(true_bools, pred_bools):\n",
    "    clf_report_optimized = classification_report(true_bools, pred_bools, target_names=label_cols, digits=5, zero_division=0, output_dict=True)\n",
    "    micro_avg = clf_report_optimized['micro avg']\n",
    "    f1 = f1_score(true_bools, pred_bools,average='micro')*100\n",
    "    acc = accuracy_score(true_bools, pred_bools)*100\n",
    "    precision = micro_avg['precision']*100\n",
    "    recall = micro_avg['recall']*100\n",
    "    \n",
    "    return f1, acc, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingFace_model_name = \"flaubert/flaubert_base_cased\"\n",
    "dataset_name = \"my dataset\" # must be changed\n",
    "model_name = \"FlauBERT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(huggingFace_model_name, num_labels=num_labels)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_classification = torch.optim.AdamW(model.parameters(),lr=2e-5)\n",
    "classification_criterion = BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "\n",
    "best_dev_f1_sgo = -1.0\n",
    "best_dev_f1_si = -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wandb\n",
    "wandb.login()\n",
    "config = {\"epochs\": epochs, \"batch_size\": batch_size, \"seq_max_length\": max_length,\n",
    "          \"lr_cls\": 2e-5,\n",
    "         \"optimizer\": \"AdamW\"}\n",
    "config.update({\"dataset\": dataset_name})\n",
    "\n",
    "# mode = \"disabled\"\n",
    "wandb.init(project=\"myProject\", entity=\"myEntity\", name=\"RunName\", config=config) # project info must be modified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trange is a tqdm wrapper around the normal python range\n",
    "for epoch_num in trange(epochs, desc=\"Epoch\", position=0):\n",
    "    \n",
    "    for phase in tqdm(['train', 'dev'], leave=False, desc='Phases', position=0):\n",
    "\n",
    "        # Tracking variables\n",
    "        true_labels,pred_labels = [], [] \n",
    "        epoch_loss = 0 # running loss\n",
    "        epoch_steps = 0\n",
    "        \n",
    "        if phase == 'train': \n",
    "            model.train()\n",
    "            \n",
    "        if phase == 'dev':\n",
    "            model.eval()\n",
    "            \n",
    "        for step, batch in enumerate(tqdm(dataloaders[phase], leave=False, desc=f\"{phase.capitalize()} Dataloader\", position=0)):\n",
    "\n",
    "            # Add batch to device\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            # Forward pass for multilabel classification\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(b_input_ids, attention_mask=b_input_mask)[0]\n",
    "                classification_logits = outputs\n",
    "                classification_logits = torch.sigmoid(classification_logits) # apply sigmoid activation function to get classification probabilities\n",
    "                \n",
    "            del b_input_ids, b_input_mask, outputs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            #loss calculation\n",
    "            classification_loss = classification_criterion(classification_logits, b_labels.type_as(classification_logits))\n",
    "            \n",
    "            if phase == 'train': \n",
    "\n",
    "                # Clear out the gradients \n",
    "                optimizer_classification.zero_grad()\n",
    "                \n",
    "                # Backward pass\n",
    "                classification_loss.backward()\n",
    "                    \n",
    "                # Update parameters and take a step using the computed gradient\n",
    "                optimizer_classification.step()\n",
    "\n",
    "            # Update tracking variables\n",
    "            epoch_loss += classification_loss.item()\n",
    "            epoch_steps += 1\n",
    "            \n",
    "            # Update Epoch Metrics\n",
    "            pred_label = classification_logits.detach().to('cpu').numpy()\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "            true_labels.append(b_labels)\n",
    "            pred_labels.append(pred_label)\n",
    "            \n",
    "\n",
    "\n",
    "        # Get Epoch Loss\n",
    "        epoch_loss = epoch_loss/epoch_steps\n",
    "        loss = {\n",
    "            'epoch_loss': epoch_loss\n",
    "        }\n",
    "        wandb.log({f'{phase.capitalize()}': loss}, commit=False)\n",
    "        \n",
    "        # Get Epoch Metrics\n",
    "        \n",
    "        # Flatten outputs\n",
    "        pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "        true_labels = [item for sublist in true_labels for item in sublist]\n",
    "        \n",
    "        train_threshold, th_micro_f1, th_acc, th_precision, th_recall = best_threshold(true_labels, pred_labels)\n",
    "        train_thresholds, ths_micro_f1, ths_acc, ths_precision , ths_recall = best_thresholds(true_labels, pred_labels)\n",
    "\n",
    "        true_bools = true_labels \n",
    "\n",
    "        # SGO Metrics \n",
    "        pred_bools = [pl>train_threshold for pl in pred_labels] \n",
    "        f1_sgo, acc_sgo, precision_sgo, recall_sgo = get_metrics(true_bools, pred_bools)\n",
    "\n",
    "        \n",
    "        # SI Metrics\n",
    "        pred_bools = [ [e>thresh for e,thresh in zip(pl,train_thresholds)] for pl in pred_labels] \n",
    "        f1_si, acc_si, precision_si, recall_si = get_metrics(true_bools, pred_bools)\n",
    "\n",
    "\n",
    "        # Log Epoch Metrics\n",
    "        metrics_sgo = {\n",
    "            'F1_score': f1_sgo,\n",
    "            'Accuracy': acc_sgo,\n",
    "            'Precision': precision_sgo,\n",
    "            'Recall': recall_sgo\n",
    "        }\n",
    "        wandb.log({f'{phase.capitalize()} SGO': metrics_sgo}, commit=False)\n",
    "        # print_results(\"SGO\", f1_sgo, acc_sgo, precision_sgo, recall_sgo)\n",
    "        \n",
    "        metrics_si = {\n",
    "            'F1_score': f1_si,\n",
    "            'Accuracy': acc_si,\n",
    "            'Precision': precision_si,\n",
    "            'Recall': recall_si\n",
    "        }\n",
    "        wandb.log({f'{phase.capitalize()} SI': metrics_si}, commit=False)\n",
    "        # print_results(\"SI\", f1_si, acc_si, precision_si, recall_si)\n",
    "        \n",
    "        \n",
    "        # Save model if valid performances are better\n",
    "        if phase == 'dev':\n",
    "            if f1_sgo > best_dev_f1_sgo :\n",
    "                best_dev_f1_sgo = f1_sgo\n",
    "                torch.save(model.state_dict(), 'state_dicts/'+model_name+'_'+dataset_name+'_best_model_weights_sgo.pt')\n",
    "\n",
    "            if f1_si > best_dev_f1_si :\n",
    "                best_dev_f1_si = f1_si\n",
    "                torch.save(model.state_dict(), 'state_dicts/'+model_name+'_'+dataset_name+'_best_model_weights_si.pt')\n",
    "\n",
    "\n",
    "    wandb.log(data={}, commit=True)\n",
    "    \n",
    "# save last model\n",
    "torch.save(model.state_dict(), 'state_dicts/last_'+ model_name +'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
